{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pymongo\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pprint\n",
    "#import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conn = \"mongodb://localhost:27017\"\n",
    "client = pymongo.MongoClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = client.realtyaus\n",
    "collection = db.main_pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(list(collection.find()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_data(data):\n",
    "    '''\n",
    "    Convert html data from realtyaus database into dataframe by obtaining key elements from \n",
    "    each webpage.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    #initiate beautiful soup html.parser\n",
    "    soup = bs(data, 'html.parser')\n",
    "    \n",
    "    #utilized find_all to obtain elements location\n",
    "    listings = soup.find_all(class_='articleset listings colset_4')\n",
    "    mlsids = listings[0].find_all('li', class_=\"data-mlsid\")\n",
    "    subdivs = [x.get_text(strip=True) for x in listings[0].find_all('li', class_=\"data-subdivision\")]\n",
    "    address = [x.get_text() for x in listings[0].find_all('li', class_=\"data-address\")]\n",
    "    locs = listings[0].find_all('li', class_=\"data-location\")\n",
    "    beds = listings[0].find_all('span', class_=\"details-bedrooms\")\n",
    "    baths = listings[0].find_all('span', class_=\"details-bathrooms\")\n",
    "    sqfts = listings[0].find_all('span', class_=\"details-sqft\")\n",
    "    acres = listings[0].find_all('span', class_=\"details-acres\")\n",
    "    dtypes = listings[0].find_all('li', class_=\"data-type\")\n",
    "    prices = listings[0].find_all('li', class_=\"data-price\")\n",
    "    status = listings[0].find_all('li', class_=\"data-status\")\n",
    "    statusexts = listings[0].find_all('li', class_=\"data-statusextra\")\n",
    "    photos = listings[0].find_all(class_=\"photo\")\n",
    "    \n",
    "    #list comprehension \n",
    "    mlsid = [x.get_text(strip=True) for x in mlsids]\n",
    "    loc = [x.get_text(strip=True) for x in locs]\n",
    "    bed = [x.get_text(strip=True) for x in beds]\n",
    "    bath = [x.get_text(strip=True) for x in baths]\n",
    "    sqft = [x.get_text(strip=True) for x in sqfts]\n",
    "    acre = [x.get_text(strip=True) for x in acres]\n",
    "    dtype = [x.get_text(strip=True) for x in dtypes]\n",
    "    price = [x.get_text(strip=True) for x in prices]\n",
    "    statu = [x.get_text(strip=True) for x in status]\n",
    "    statusext = [x.get_text(strip=True) for x in statusexts]\n",
    "    photo = [x.img['data-src'] for x in photos]\n",
    "    \n",
    "    #create a dictionary \n",
    "    data = {\n",
    "        'mlsid': mlsid,\n",
    "        'subdiv': subdiv,\n",
    "        'location': loc,\n",
    "        'bedrooms': bed,\n",
    "        'bathrooms': bath, \n",
    "        'sqft': sqft,\n",
    "        'acre': acre,\n",
    "        'data_type': dtype,\n",
    "        'price': price,\n",
    "        'status': statu,\n",
    "        'status_extra': statusext,\n",
    "        'photo': photo\n",
    "    }\n",
    "    \n",
    "    #convert dictionary into pandas dataframe, used pd.Series to assure all parameter lengths are equal\n",
    "    df = pd.DataFrame([pd.Series(value, name=k) for k, value in data.items()]).T\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initiate beautiful soup html.parser\n",
    "soup = bs(df['html'][0], 'html.parser')\n",
    "\n",
    "#utilized find_all to obtain elements location\n",
    "listings = soup.find_all(class_='articleset listings colset_4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Austin, TX\\xa078704',\n",
       " 'Austin, TX\\xa078756',\n",
       " 'Dripping Springs, TX\\xa078620',\n",
       " 'Austin, TX\\xa078737',\n",
       " 'Buda, TX\\xa078610',\n",
       " 'Austin, TX\\xa078749',\n",
       " 'Spicewood, TX\\xa078669',\n",
       " 'Austin, TX\\xa078734',\n",
       " 'Leander, TX\\xa078641',\n",
       " 'Austin, TX\\xa078734',\n",
       " 'Leander, TX\\xa078641',\n",
       " 'Austin, TX\\xa078737',\n",
       " 'Georgetown, TX\\xa078626',\n",
       " 'Austin, TX\\xa078717',\n",
       " 'Austin, TX\\xa078729',\n",
       " 'Round Rock, TX\\xa078665',\n",
       " 'Pflugerville, TX\\xa078660',\n",
       " 'Austin, TX\\xa078746',\n",
       " 'West Lake Hills, TX\\xa078746',\n",
       " 'Austin, TX\\xa078731',\n",
       " 'Austin, TX\\xa078759',\n",
       " 'Austin, TX\\xa078737',\n",
       " 'Liberty Hill, TX\\xa078642',\n",
       " 'Austin, TX\\xa078723']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x.get_text() for x in listings[0].find_all('li', class_=\"data-location\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df[['html', 'time_scraped']]\n",
    "\n",
    "dataframe = []\n",
    "for x in data['html']:\n",
    "    dt = convert_data(x)\n",
    "    dt['time'] = data['time_scraped']\n",
    "    dataframe.append(dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_df = pd.concat(dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 17310 entries, 0 to 17309\n",
      "Data columns (total 13 columns):\n",
      "mlsid           17310 non-null object\n",
      "subdiv          17310 non-null object\n",
      "location        17310 non-null object\n",
      "bedrooms        17288 non-null object\n",
      "bathrooms       17302 non-null object\n",
      "sqft            17304 non-null object\n",
      "acre            12939 non-null object\n",
      "data_type       17310 non-null object\n",
      "price           17310 non-null object\n",
      "status          17310 non-null object\n",
      "status_extra    17310 non-null object\n",
      "photo           17310 non-null object\n",
      "time            17310 non-null object\n",
      "dtypes: object(13)\n",
      "memory usage: 1.7+ MB\n"
     ]
    }
   ],
   "source": [
    "total_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
